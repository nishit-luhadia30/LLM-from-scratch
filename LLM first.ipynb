{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0cda33-9905-45cd-b922-c16b0fbab092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1c13a5d-ec97-4256-a94e-6c7e281fbec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters int the file: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "  raw_text = f.read();\n",
    "\n",
    "print(\"Total number of characters int the file:\", len(raw_text));\n",
    "print(raw_text[:100]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07c4bb56-edb7-4631-a368-94f1f8834856",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Our goal is to tokenize this 20,479-character (which is present in the file named the-verdict.txt) short story into individual words and \n",
    "special characters that we can then turn into embeddings for LLM training\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Note that it's common to process millions of articles and hundreds of thousands of books -- many gigabytes of text -- when working with LLMs.\n",
    "However, for educational purposes, it's sufficient to work with smaller text samples like a single book to illustrate the main ideas behind \n",
    "the text processing steps and to make it possible to run it in reasonable time on consumer hardware.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "How can we best split this text to obtain a list of tokens? For this, we go on a small excursion and use Python's regular expression \n",
    "library re for illustration purposes. (Note that you don't have to learn or memorize any regular expression syntax since we will transition\n",
    "to a pre-built tokenizer later in this chapter.)\n",
    "\"\"\"\n",
    "\n",
    "# Using some simple example text, we can use the re.split command with the following syntax to split a text on whitespace characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7ef5602-1c9a-4422-baad-be11801fb047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' ', 'nishit,', ' ', ',,world!!']\n"
     ]
    }
   ],
   "source": [
    "# this re -- Regular Expression . It works like whenever in the statement it finds white-spaces it separates the words\n",
    "import re\n",
    "\n",
    "text = \"Hello nishit, ,,world!!\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177d41d5-c12c-4e68-88e6-e91c0419166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The result is a list of individual words, whitespaces, and punctuation characters:\n",
    "\n",
    "# Let's modify the regular expression splits on whitespaces (\\s) and commas, and periods ([,.]):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ba40044-20ed-4e23-b7ad-ef507c024bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      ",\n",
      "\n",
      " \n",
      "world\n",
      ".\n",
      "\n",
      " \n",
      "Is\n",
      " \n",
      "this--\n",
      " \n",
      "a\n",
      " \n",
      "test?\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "\n",
    "for item in result:\n",
    "    print(item)\n",
    "\n",
    "# We can see that the words and punctuation characters are now separate list entries just as we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e0c6b8-4eb4-4b2b-9fb8-eee180042dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A small remaining issue is that the list still includes whitespace characters. Optionally, we can remove these redundant \n",
    "# characters safely as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a118f4da-0624-4c9f-b966-304a71aae5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      ",\n",
      "world\n",
      ".\n",
      "Is\n",
      "this--\n",
      "a\n",
      "test?\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "\n",
    "for i in result:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674b151f-7242-473a-bb8c-ee49764e68cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVING WHITESPACES OR NOT\n",
    "\n",
    "\"\"\"\n",
    "When developing a simple tokenizer, whether we should encode whitespaces as separate characters or just remove them depends\n",
    "on our application and its requirements. Removing whitespaces reduces the memory and computing requirements. However, keeping \n",
    "whitespaces can be useful if we train models that are sensitive to the exact structure of the text (for example, Python code, which is\n",
    "sensitive to indentation and spacing). Here, we remove whitespaces for simplicity and brevity of the tokenized outputs. Later, we will \n",
    "switch to a tokenization scheme that includes whitespaces.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801bcfb5-459e-444e-bdcf-07aec75678bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The tokenization scheme we devised above works well on the simple sample text. Let's modify it a bit further so that it can also handle other \n",
    "types of punctuation, such as question marks, quotation marks, and the double-dashes we have seen earlier in the first 100 characters of \n",
    "Edith Wharton's short story, along with additional special characters:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "709a2558-7dc0-420f-b2ba-7b298097520a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?'] \n",
      "\n",
      "Hello\n",
      ",\n",
      "world\n",
      ".\n",
      "Is\n",
      "this\n",
      "--\n",
      "a\n",
      "test\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text); \n",
    "result = [item for item in result if item.strip()]\n",
    "\n",
    "# the above two lines indicates the tokenization lines\n",
    "\n",
    "print(result, '\\n')\n",
    "\n",
    "for item in result:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e905593-3e51-4d40-a4a8-79e9062e79d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we got a basic tokenizer working, let's apply it to Edith Wharton's entire short story:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c79ac97b-401c-447d-bc55-539ea7ad1ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5238bca-76ce-4200-8cbf-499a65296408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc599aa-f2a9-43bc-a7ab-851cb230133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Creating Token IDs\n",
    "\n",
    "\"\"\"\n",
    "In the previous section, we tokenized Edith Wharton's short story and assigned it to a Python variable called preprocessed. \n",
    "Let's now create a list of all unique tokens and sort them alphabetically to determine the vocabulary size:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "62f6aa93-d584-4c2b-bf11-83144abdfe0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed)) # set for unique words\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1d5aa0-91d2-4ae7-97d4-218494afdb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After determining that the vocabulary size is 1,130 via the above code, we create the vocabulary and print its \n",
    "# first 51 entries for illustration purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "69df8c8d-5c92-4548-9988-a60ebfd49c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer, token in enumerate (all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "44891d3f-2b89-4bc7-aa5b-94df8bfea262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 15:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "69e7b141-97a5-4f43-90e7-4a44627ba02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see, based on the output above, the dictionary contains individual tokens associated with unique integer labels.\n",
    "# Later in this book, when we want to convert the outputs of an LLM from numbers back into text, we also need a way to turn token IDs into text.\n",
    "\n",
    "# For this, we can create an inverse version of the vocabulary that maps token IDs back to corresponding text tokens.\n",
    "\n",
    "# Let's implement a complete tokenizer class in Python.\n",
    "\n",
    "# The class will have an encode method that splits text into tokens and carries out the string-to-integer mapping to produce token IDs via the vocabulary.\n",
    "\n",
    "# In addition, we implement a decode method that carries out the reverse integer-to-string mapping to convert the token IDs back into text.\n",
    "\n",
    "# Step 1: Store the vocabulary as a class attribute for access in the encode and decode methods\n",
    "\n",
    "# Step 2: Create an inverse vocabulary that maps token IDs back to the original text tokens\n",
    "\n",
    "# Step 3: Process input text into token IDs\n",
    "\n",
    "# Step 4: Convert token IDs back into text\n",
    "\n",
    "# Step 5: Replace spaces before the specified punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f018f564-19ec-4350-a3b4-e949a00bdc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "                                \n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6bccc2-68ca-4ff6-9d64-60c1f36939bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's instantiate a new tokenizer object from the SimpleTokenizerV1 class and tokenize a passage from Edith Wharton's short story\n",
    "# to try it out in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d2a76820-4aa4-499b-9746-3b27efe8cdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ab0825-b3e7-4322-88e0-62381754a7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code above prints the following token IDs: Next, let's see if we can turn these token IDs back into text using the decode method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1133c88d-6c30-43d8-a07f-038302d862b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78e9874-3f07-4ba3-9c76-5be3497b684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the output above, we can see that the decode method successfully converted the token IDs back into the original text.\n",
    "\n",
    "# So far, so good. We implemented a tokenizer capable of tokenizing and de-tokenizing text based on a snippet from the training set.\n",
    "\n",
    "# Let's now apply it to a new text sample that is not contained in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "84ce6179-c24e-4a92-9531-70e55348583e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[103]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mHello, do you like tea?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mSimpleTokenizerV1.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m      7\u001b[39m preprocessed = re.split(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m([,.:;?_!\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m]|--|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms)\u001b[39m\u001b[33m'\u001b[39m, text)\n\u001b[32m      9\u001b[39m preprocessed = [\n\u001b[32m     10\u001b[39m     item.strip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.strip()\n\u001b[32m     11\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m ids = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[31mKeyError\u001b[39m: 'Hello'"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c1eed8-fa89-4ec6-adff-61ac12beaa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The problem is that the word \"Hello\" was not used in the The Verdict short story.\n",
    "\n",
    "# Hence, it is not contained in the vocabulary.\n",
    "\n",
    "# This highlights the need to consider large and diverse training sets to extend the vocabulary when working on LLMs.\n",
    "\n",
    "# ADDING SPECIAL CONTEXT TOKENS\n",
    "# In the previous section, we implemented a simple tokenizer and applied it to a passage from the training set.\n",
    "\n",
    "# In this section, we will modify this tokenizer to handle unknown words.\n",
    "\n",
    "# In particular, we will modify the vocabulary and tokenizer we implemented in the previous section, SimpleTokenizerV2, to support \n",
    "# two new tokens, <|unk|> and <|endoftext|>\n",
    "\n",
    "# We can modify the tokenizer to use an <|unk|> token if it encounters a word that is not part of the vocabulary.\n",
    "\n",
    "# Furthermore, we add a token between unrelated texts.\n",
    "\n",
    "# For example, when training GPT-like LLMs on multiple independent documents or books, it is common to insert a token before each \n",
    "# document or book that follows a previous text source\n",
    "\n",
    "# Let's now modify the vocabulary to include these two special tokens, and <|endoftext|>, by adding these to the list of all \n",
    "# unique words that we created in the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d6a776c0-4bd1-4e7c-9c4b-5941f55610b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6c3b0c2c-9f9d-4f94-9e65-ea7e60b50b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6d54fd-9bf9-4ad8-b051-f30c57e5b7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the output of the print statement above, the new vocabulary size is 1132 (the vocabulary size in the previous section was 1130).\n",
    "\n",
    "# As an additional quick check, let's print the last 5 entries of the updated vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0b40d212-51c5-447a-8448-bc631e562505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ac5c53-917a-4981-b37c-bf907ef977d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple text tokenizer that handles unknown words\n",
    "\n",
    "# Step 1: Replace unknown words by <|unk|> tokens\n",
    "\n",
    "# Step 2: Replace spaces before the specified punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "95983486-2271-4eb5-bfcd-057826e92a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4b93239e-a1f1-4b95-8ce2-ea5c1d1556b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f9ee1ea5-2430-4ed1-9845-6ffde20f2aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n"
     ]
    }
   ],
   "source": [
    "ids2 = tokenizer.encode(text)\n",
    "print(ids2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "507a46ff-65d1-41f7-b004-81ef70c55db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f09a154-436b-45e0-899a-301cf5d1761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on comparing the de-tokenized text above with the original input text, we know that the training dataset, Edith Wharton's\n",
    "# short story The Verdict, did not contain the words \"Hello\" and \"palace.\"\n",
    "\n",
    "# So far, we have discussed tokenization as an essential step in processing text as input to LLMs. Depending on the LLM, some \n",
    "# researchers also consider additional special tokens such as the following:\n",
    "\n",
    "# [BOS] (beginning of sequence): This token marks the start of a text. It signifies to the LLM where a piece of content begins.\n",
    "\n",
    "# [EOS] (end of sequence): This token is positioned at the end of a text, and is especially useful when concatenating \n",
    "# multiple unrelated texts, similar to <|endoftext|>. For instance, when combining two different Wikipedia articles or \n",
    "# books, the [EOS] token indicates where one article ends and the next one begins.\n",
    "\n",
    "# [PAD] (padding): When training LLMs with batch sizes larger than one, the batch might contain texts of varying lengths. \n",
    "# To ensure all texts have the same length, the shorter texts are extended or \"padded\" using the [PAD] token, up to the \n",
    "# length of the longest text in the batch.\n",
    "\n",
    "# Note that the tokenizer used for GPT models does not need any of these tokens mentioned above but only uses an \n",
    "# <|endoftext|> token for simplicity\n",
    "\n",
    "# the tokenizer used for GPT models also doesn't use an <|unk|> token for outof-vocabulary words. Instead, \n",
    "# GPT models use a byte pair encoding tokenizer, which breaks down words into subword units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd35cfa7-a2df-4ec3-817e-0706558b4aed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
