{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0cda33-9905-45cd-b922-c16b0fbab092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1c13a5d-ec97-4256-a94e-6c7e281fbec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters int the file: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "  raw_text = f.read();\n",
    "\n",
    "print(\"Total number of characters int the file:\", len(raw_text));\n",
    "print(raw_text[:100]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07c4bb56-edb7-4631-a368-94f1f8834856",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Our goal is to tokenize this 20,479-character (which is present in the file named the-verdict.txt) short story into individual words and \n",
    "special characters that we can then turn into embeddings for LLM training\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Note that it's common to process millions of articles and hundreds of thousands of books -- many gigabytes of text -- when working with LLMs.\n",
    "However, for educational purposes, it's sufficient to work with smaller text samples like a single book to illustrate the main ideas behind \n",
    "the text processing steps and to make it possible to run it in reasonable time on consumer hardware.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "How can we best split this text to obtain a list of tokens? For this, we go on a small excursion and use Python's regular expression \n",
    "library re for illustration purposes. (Note that you don't have to learn or memorize any regular expression syntax since we will transition\n",
    "to a pre-built tokenizer later in this chapter.)\n",
    "\"\"\"\n",
    "\n",
    "# Using some simple example text, we can use the re.split command with the following syntax to split a text on whitespace characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7ef5602-1c9a-4422-baad-be11801fb047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' ', 'nishit,', ' ', ',,world!!']\n"
     ]
    }
   ],
   "source": [
    "# this re -- Regular Expression . It works like whenever in the statement it finds white-spaces it separates the words\n",
    "import re\n",
    "\n",
    "text = \"Hello nishit, ,,world!!\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177d41d5-c12c-4e68-88e6-e91c0419166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The result is a list of individual words, whitespaces, and punctuation characters:\n",
    "\n",
    "# Let's modify the regular expression splits on whitespaces (\\s) and commas, and periods ([,.]):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ba40044-20ed-4e23-b7ad-ef507c024bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      ",\n",
      "\n",
      " \n",
      "world\n",
      ".\n",
      "\n",
      " \n",
      "Is\n",
      " \n",
      "this--\n",
      " \n",
      "a\n",
      " \n",
      "test?\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "\n",
    "for item in result:\n",
    "    print(item)\n",
    "\n",
    "# We can see that the words and punctuation characters are now separate list entries just as we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e0c6b8-4eb4-4b2b-9fb8-eee180042dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A small remaining issue is that the list still includes whitespace characters. Optionally, we can remove these redundant \n",
    "# characters safely as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a118f4da-0624-4c9f-b966-304a71aae5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'nishit', ',', ',', ',', 'world!!']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674b151f-7242-473a-bb8c-ee49764e68cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVING WHITESPACES OR NOT\n",
    "\n",
    "\"\"\"\n",
    "When developing a simple tokenizer, whether we should encode whitespaces as separate characters or just remove them depends\n",
    "on our application and its requirements. Removing whitespaces reduces the memory and computing requirements. However, keeping \n",
    "whitespaces can be useful if we train models that are sensitive to the exact structure of the text (for example, Python code, which is\n",
    "sensitive to indentation and spacing). Here, we remove whitespaces for simplicity and brevity of the tokenized outputs. Later, we will \n",
    "switch to a tokenization scheme that includes whitespaces.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801bcfb5-459e-444e-bdcf-07aec75678bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The tokenization scheme we devised above works well on the simple sample text. Let's modify it a bit further so that it can also handle other \n",
    "types of punctuation, such as question marks, quotation marks, and the double-dashes we have seen earlier in the first 100 characters of \n",
    "Edith Wharton's short story, along with additional special characters:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "709a2558-7dc0-420f-b2ba-7b298097520a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?'] \n",
      "\n",
      "Hello\n",
      ",\n",
      "world\n",
      ".\n",
      "Is\n",
      "this\n",
      "--\n",
      "a\n",
      "test\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text); \n",
    "result = [item for item in result if item.strip()]\n",
    "\n",
    "# the above two lines indicates the tokenization lines\n",
    "\n",
    "print(result, '\\n')\n",
    "\n",
    "for item in result:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c79ac97b-401c-447d-bc55-539ea7ad1ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5238bca-76ce-4200-8cbf-499a65296408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
